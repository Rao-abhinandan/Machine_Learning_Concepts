{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Supervised learning - Addreses classfication and predictive analytics problem (Predict outcome with high probability)\n",
    "\n",
    "Linearly seperable\n",
    "Mathematically a Sigmoid function (S function) f(x)= 1/ (1+e^-x)  (probaility of success / probability of failure),  which maps data or x values between 0 and 1 \n",
    "Converts continous data into probabaility of categories (discrete values) !\n",
    "\n",
    "Types : \n",
    "Binary logistic regression - 2 possible outcomes\n",
    "Multinomial logistic regression -  More than 2 possible outcomes but not in order\n",
    "Ordinal logistic regression -  More than 2 possible outcomes but have defined order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometric Intuition\n",
    "\n",
    "Decision Boundary - The line or curve that separates the two classes in the feature space. In other words, it is the boundary that separates the regions of the feature space where each class is assigned.\n",
    "\n",
    "Logistic regression finds a hyperplane in the feature space that separates the two classes. Points on one side of the hyperplane are classified as one class, and points on the other side as the opposite class.\n",
    "For example:\n",
    "In 2D, the hyperplane is a line.\n",
    "In 3D, the hyperplane is a plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Function to demonstrate the sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Plot the sigmoid function\n",
    "def plot_sigmoid():\n",
    "    z = np.linspace(-10, 10, 100)\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(z, y, label='Sigmoid Function')\n",
    "    plt.axvline(0, color='gray', linestyle='--', label='Decision Boundary (z=0)')\n",
    "    plt.title(\"Sigmoid Function\")\n",
    "    plt.xlabel(\"z\")\n",
    "    plt.ylabel(\"Sigmoid(z)\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "def create_dataset():\n",
    "    X, y = make_classification(\n",
    "        n_samples=100, n_features=2, n_informative=2, n_redundant=0, \n",
    "        n_clusters_per_class=1, class_sep=1.5, random_state=42\n",
    "    )\n",
    "    return X, y\n",
    "\n",
    "# Train logistic regression and plot decision boundary\n",
    "def plot_decision_boundary(X, y):\n",
    "    # Train logistic regression\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Plot the dataset\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], color='blue', label='Class 0')\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], color='red', label='Class 1')\n",
    "    \n",
    "    # Plot the decision boundary\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, alpha=0.2, cmap='coolwarm')\n",
    "    plt.axhline(0, color='gray', linestyle='--')\n",
    "    plt.axvline(0, color='gray', linestyle='--')\n",
    "    plt.title(\"Logistic Regression Decision Boundary\")\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Plot the sigmoid function\n",
    "    print(\"Visualizing the Sigmoid Function...\")\n",
    "    plot_sigmoid()\n",
    "    \n",
    "    # Create dataset and visualize decision boundary\n",
    "    print(\"Creating Dataset and Plotting Decision Boundary...\")\n",
    "    X, y = create_dataset()\n",
    "    plot_decision_boundary(X, y)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics to be updated : \n",
    "Assumptions\n",
    "Mathematical intutiion\n",
    "Performance improvement - Regularisation\n",
    "Model performance / goodness of fit (Accuracy precision Recall F1 score AUC-ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GENAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
